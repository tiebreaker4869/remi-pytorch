{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center> <span style=\"font-family:serif; font-size:34px;\"> CSE 253 Assignment 2</span> </center>**\n",
    "\n",
    "<style>\n",
    "body {\n",
    "  font-family: 'Times New Roman', sans-serif;\n",
    "  font-size: 20pt;\n",
    "}\n",
    "h1 { font-weight: bold !important; }\n",
    "p {\n",
    "  font: 16pt 'Times New Roman' !important;\n",
    "}\n",
    "\n",
    "div.text_cell_render h1, .rendered_html h1 {\n",
    "  color: #126dce !important;\n",
    "  font-size: 2.2em !important;\n",
    "  font-weight: bold !important;\n",
    "  margin: 0.4em 0 0.3em 0 !important;\n",
    "}\n",
    "div.text_cell_render h2, .rendered_html h2 {\n",
    "  color: #126dce !important;\n",
    "  font-size: 1.7em !important;\n",
    "  font-weight: bold !important;\n",
    "  margin: 0.3em 0 0.2em 0 !important;\n",
    "}\n",
    "div.text_cell_render h3, .rendered_html h3 {\n",
    "  color: #126dce !important;\n",
    "  font-size: 1.35em !important;\n",
    "  font-weight: bold !important;\n",
    "  margin: 0.3em 0 0.1em 0 !important;\n",
    "}\n",
    "div.text_cell_render h4, .rendered_html h4 {\n",
    "  color: #126dce !important;\n",
    "  font-size: 1.1em !important;\n",
    "  font-weight: bold !important;\n",
    "}\n",
    "div.text_cell_render h5, .rendered_html h5 {\n",
    "  color: #126dce !important;\n",
    "  font-size: 1em !important;\n",
    "  font-weight: bold !important;\n",
    "}\n",
    "\n",
    "/* Markdown 代码块（行内、块） */\n",
    "div.text_cell_render pre, div.text_cell_render code,\n",
    ".rendered_html pre, .rendered_html code {\n",
    "  font-family: 'Fira Mono', 'Consolas', monospace !important;\n",
    "  font-size: 13pt !important;\n",
    "  color: #353535 !important;\n",
    "  background: #efefef !important;\n",
    "  border-radius: 2px !important;\n",
    "  border-left: 3px solid rgba(240,147,43,.50) !important;\n",
    "  padding: 5px 10px !important;\n",
    "  margin: 6px 0 !important;\n",
    "  line-height: 1.6 !important;\n",
    "  max-width: 90%;\n",
    "}\n",
    "\n",
    "/* Markdown 内联代码高亮（如 `code`） */\n",
    "div.text_cell_render code, .rendered_html code {\n",
    "  background: #efefef !important;\n",
    "  color: #d6336c !important;\n",
    "  padding: 2px 5px !important;\n",
    "  border-radius: 2px !important;\n",
    "  font-size: 1em !important;\n",
    "  font-family: 'Fira Mono', 'Consolas', monospace !important;\n",
    "}\n",
    "\n",
    "/* 表格美化 */\n",
    ".rendered_html table, div.text_cell_render table {\n",
    "  font-family: 'Times New Roman', sans-serif !important;\n",
    "  font-size: 1em !important;\n",
    "  color: #353535 !important;\n",
    "  background: #fff !important;\n",
    "  border-collapse: collapse !important;\n",
    "  border: 1px solid #e7e7e7 !important;\n",
    "  margin: 1em 0 !important;\n",
    "  width: auto !important;\n",
    "}\n",
    ".rendered_html th, .rendered_html td, div.text_cell_render th, div.text_cell_render td {\n",
    "  border: 1px solid #e7e7e7 !important;\n",
    "  padding: 8px 12px !important;\n",
    "  text-align: left !important;\n",
    "  vertical-align: middle !important;\n",
    "}\n",
    ".rendered_html thead, div.text_cell_render thead {\n",
    "  background: #ebebeb !important;\n",
    "  color: #126dce !important;\n",
    "  font-weight: bold !important;\n",
    "}\n",
    ".rendered_html tbody tr:nth-child(odd), div.text_cell_render tbody tr:nth-child(odd) {\n",
    "  background: #fafafa !important;\n",
    "}\n",
    ".rendered_html tbody tr:hover, div.text_cell_render tbody tr:hover {\n",
    "  background: #e6f7ff !important;\n",
    "}\n",
    "\n",
    "/* 列表、段落间距微调 */\n",
    ".rendered_html * + p {\n",
    "  margin-top: 0.5em !important;\n",
    "  margin-bottom: 0.5em !important;\n",
    "}\n",
    "\n",
    "/* 让无序和有序列表更紧凑 */\n",
    ".rendered_html ul, .rendered_html ol, \n",
    "div.text_cell_render ul, div.text_cell_render ol {\n",
    "    margin-top: 0.08em !important;\n",
    "    margin-bottom: 0.08em !important;\n",
    "    padding-left: 2em !important;\n",
    "}\n",
    "\n",
    ".rendered_html ul li, .rendered_html ol li,\n",
    "div.text_cell_render ul li, div.text_cell_render ol li {\n",
    "    margin-top: 0 !important;\n",
    "    margin-bottom: 0 !important;\n",
    "    padding-top: 0 !important;\n",
    "    padding-bottom: 0 !important;\n",
    "    line-height: 1.05 !important;   /* 极致紧凑 */\n",
    "}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <span style=\"font-family:semi-serif; font-size:28px;\"> Overview</span></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import miditoolkit\n",
    "import pretty_midi\n",
    "from midi2audio import FluidSynth\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <span style=\"font-family:semi-serif; font-size:28px;\"> Task 1: Unconditional Symbolic Music Generation</span></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook only covers key portions of our implementation. For complete implementation and other scripts used in this project, refer to this anonymous repository: [https://anonymous.4open.science/r/remi-pytorch-6C7D](https://anonymous.4open.science/r/remi-pytorch-6C7D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:28px;\"> 1. Introduction</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:24px;\"> 1.1 Task and Context</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Where does your dataset come from? What is it for, how was it collected. etc -->\n",
    "\n",
    "**Where is the dataset from**: In our project, we used the pop piano music dataset **POP1K7** published by [Hsiao et al. 2021](https://arxiv.org/pdf/2101.02402). This dataset contains 1,748 pop piano tracks, 108 hours in total, all in 4/4 time signature, originally collected from the Internet.\n",
    "\n",
    "**What is the dataset for**: The dataset is used for automatic **Pop piano composition (unconditional symbolic music generation)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:24px;\"> 1.2 Data Preprocessing and Loading</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is utilized in its processed form as provided by the original authors:\n",
    "\n",
    "- **Transcription**: The state-of-the-art RNN model for automatic piano transcription, [\"Onset and Frames\" (Hawthorne et al. 2018a)](https://arxiv.org/abs/1710.11153), is employed to estimate the pitch, onset and offset times, as well as the velocity of musical notes from audio.\n",
    "\n",
    "- **Synchronization**: To convert wall clock time into symbolic timing, the RNN-based model from the Python package [madmom (Bock et al. 2016)](https://arxiv.org/abs/1605.07008) is used to estimate downbeat and beat positions, representing the state-of-the-art for this task. Subsequently, 480 ticks are interpolated between each pair of adjacent beats, and absolute time is mapped to the corresponding tick. This approach preserves fine timing accuracy. Tempo changes are inferred based on the intervals between consecutive beats.\n",
    "\n",
    "- **Analysis**: For the conditional generation task, melody notes and chord symbols are estimated from the transcription results to construct lead sheets. A rule-based chord recognition algorithm is developed to identify 12 chord roots and 7 chord qualities. The “Skyline algorithm” is applied to extract melodies. Since lead sheets are typically in a coarser time resolution, both chord symbols and melody notes are quantized to quarter notes (i.e., beat times).\n",
    "\n",
    "- **Quantization**: Tempo, velocity, duration, and beat positions are quantized to reduce the overall vocabulary size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/imgs/data_proc_diagram.png\"/> \n",
    "\n",
    "*Figure 1: POP1K7 Data Processing Pipeline, originally from [Compound Word Transformer\n",
    "](https://github.com/YatingMusic/compound-word-transformer/blob/main/dataset/Dataset.md)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:24px;\"> 1.3 Exploratory Analysis</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze several musical features for each MIDI file, including:\n",
    "\n",
    "- **Pitch density**: the ratio of unique pitches to the total number of notes, reflecting melodic diversity.\n",
    "\n",
    "- **Pitch range**: the difference between the highest and lowest note, indicating the span of pitches used.\n",
    "\n",
    "- **Note density**: the number of notes per second, representing how densely notes are distributed in time.\n",
    "\n",
    "- **Average pitch interval**: the mean distance between consecutive note pitches, describing the typical melodic movement.\n",
    "\n",
    "- **Average inter-onset interval (IOI)**: the average time gap between the start times of adjacent notes, characterizing the rhythmic pacing.\n",
    "\n",
    "These features together capture key aspects of a piece’s melodic and rhythmic structure, providing a quantitative summary of its musical characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below presents the statistics of the dataset.\n",
    "\n",
    "| Feature             | Mean     | Median   | Min     | Max     | Std      | N    |\n",
    "|---------------------|----------|----------|---------|---------|----------|------|\n",
    "| pitch_density       | 0.0319   | 0.0288   | 0.0081  | 0.1395  | 0.0141   | 1747 |\n",
    "| pitch_range         | 54.4648  | 53.0000  | 28.0000 | 80.0000 | 8.9902   | 1747 |\n",
    "| avg_pitch_interval  | 13.0434  | 12.8085  | 6.9252  | 21.7081 | 2.1046   | 1747 |\n",
    "| note_density        | 6.0185   | 5.6158   | 2.0360  | 20.4708 | 2.1945   | 1747 |\n",
    "| avg_ioi             | 0.1823   | 0.1742   | 0.0476  | 0.4813  | 0.0594   | 1747 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/pop1k7_analysis/avg_ioi.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/pop1k7_analysis/avg_pitch_interval.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/pop1k7_analysis/note_density.png\" width=\"350\"/>\n",
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/pop1k7_analysis/pitch_density.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/pop1k7_analysis/pitch_range.png\" width=\"350\"/> \n",
    "\n",
    "*Figure 2: Music Feature Distribution of POP1K7*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key portions of data analysis script is attach below, which implemented music feature extraction:\n",
    "\n",
    "```python\n",
    "def extract_features(pm: pretty_midi.PrettyMIDI) -> Dict[str, float]:\n",
    "    notes = [note for instr in pm.instruments for note in instr.notes if not instr.is_drum]\n",
    "    if not notes:\n",
    "        return None\n",
    "\n",
    "    pitches = [note.pitch for note in notes]\n",
    "    pitch_count = len(set(pitches))\n",
    "    pitch_range = max(pitches) - min(pitches)\n",
    "    pitch_intervals = [abs(pitches[i] - pitches[i - 1]) for i in range(1, len(pitches))]\n",
    "    avg_pitch_interval = np.mean(pitch_intervals) if pitch_intervals else 0\n",
    "\n",
    "    onset_times = sorted([note.start for note in notes])\n",
    "    ioi = [onset_times[i] - onset_times[i - 1] for i in range(1, len(onset_times))]\n",
    "    avg_ioi = np.mean(ioi) if ioi else 0\n",
    "\n",
    "    start_time = min([note.start for note in notes])\n",
    "    end_time = max([note.end for note in notes])\n",
    "    duration = end_time - start_time\n",
    "    duration = duration if duration > 0 else 1  # avoid div by zero\n",
    "\n",
    "    note_density = len(notes) / duration\n",
    "    pitch_density = pitch_count / len(notes) if len(notes) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"pitch_density\": pitch_density,\n",
    "        \"pitch_range\": pitch_range,\n",
    "        \"avg_pitch_interval\": avg_pitch_interval,\n",
    "        \"note_density\": note_density,\n",
    "        \"avg_ioi\": avg_ioi\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:serif; font-size:28px;\"> 2. Modeling</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:24px;\"> 2.1 ML Formulation</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We formulate expressive pop piano music generation as an **autoregressive sequence modeling problem** over a discrete vocabulary of symbolic music events.\n",
    "\n",
    "Let  \n",
    "- $\\mathcal{V}$ denote the vocabulary of REMI events (Bar, Position, Note-On, Note Duration, Note Velocity, Tempo, Chord, etc.);\n",
    "- a song is represented as an event sequence $\\mathbf{x} = (x_1, x_2, ..., x_T)$, where $x_t \\in \\mathcal{V}$.\n",
    "\n",
    "The goal is to model the joint probability of the sequence:\n",
    "$$\n",
    "P(\\mathbf{x}) = P(x_1, x_2, ..., x_T)\n",
    "$$\n",
    "which, by the chain rule, is factorized as:\n",
    "$$\n",
    "P(\\mathbf{x}) = \\prod_{t=1}^T P(x_t \\mid x_1, ..., x_{t-1})\n",
    "$$\n",
    "\n",
    "A neural network is trained to estimate the conditional probability:\n",
    "$$\n",
    "P_\\theta(x_t \\mid x_1, ..., x_{t-1})\n",
    "$$\n",
    "where $\\theta$ denotes the model parameters.\n",
    "\n",
    "During training, the objective is to minimize the negative log-likelihood (or cross-entropy loss):\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\sum_{t=1}^T \\log P_\\theta(x_t \\mid x_1, ..., x_{t-1})\n",
    "$$\n",
    "\n",
    "**Generation:**  \n",
    "At inference time, given a (possibly empty) seed sequence, the model generates a new composition by sampling each event $x_t$ autoregressively from the learned distribution until an end-of-piece token or a maximum length is reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:24px;\"> 2.2 Discussion on Model Selection</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miditok.readthedocs.io/en/latest/_images/remi.png\"/>\n",
    "\n",
    "*Figure 3: The REMI event representation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experiments, we adopt Transformer-based models as the primary approach for symbolic music generation, using a [**REMI**](https://arxiv.org/abs/2002.00212) tokenization scheme to represent musical events. For comparison, we implement Markov chain and LSTM-based models as baseline methods.\n",
    "\n",
    "**Markov Chain models**:\n",
    "<ul class=\"key-features\"> \n",
    "    <li><strong>Advantages:</strong> Simple and computationally efficient, making them well-suited as baseline models.</li> \n",
    "    <li><strong>Disadvantages:</strong> Unable to capture long-range dependencies or global musical structure, often resulting in repetitive and locally constrained outputs.</li> \n",
    "</ul>\n",
    "\n",
    "**LSTM-based models**:\n",
    "<ul class=\"key-features\">\n",
    "    <li><strong>Advantages:</strong> Maintain a recurrent hidden state, which allows them to capture longer-term dependencies and musical motifs.</li> \n",
    "    <li><strong>Disadvantages:</strong> Still limited in modeling very long-range structure. The sequential nature of LSTM computations also limits efficiency and scalability.</li> \n",
    "</ul>\n",
    "\n",
    "**Transformer-based models**:\n",
    "<ul class=\"key-features\"> \n",
    "    <li><strong>Advantages:</strong> Leverage self-attention mechanisms to capture both local and global dependencies across a sequence, enabling the generation of music with coherent long-term structure. Their parallelizable architecture also makes them more scalable than RNN-based models.</li> \n",
    "    <li><strong>Disadvantages:</strong> Require significantly greater computational resources and are more complex to implement, especially for long sequences.</li> \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:24px;\"> 2.3 Codes</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we walk through the key portions of our transformer-based method implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family:serif; font-size:20px;\"> 2.3.1 Tokenization</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our pipeline, we use the REMI tokenization scheme to convert raw MIDI files into discrete symbolic event sequences, which is then used by the transformer model.\n",
    "\n",
    "Key steps:\n",
    "- **Extracting events**: Each MIDI file is parsed and quantized into REMI events, optionally including chords.\n",
    "\n",
    "- **Event-to-token mapping**: Every event is converted into an integer using the vocabulary dictionary.\n",
    "\n",
    "- **Sequence segmentation**: Long token sequences are split into overlapping segments of fixed length (e.g., 512 tokens per segment), forming input-output pairs for training.\n",
    "\n",
    "\n",
    "```python\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, train = True, midi_l = [], dict_pth = './dictionary/dictionary_REMI-tempo-checkpoint.pkl'):\n",
    "        self.midi_l = midi_l\n",
    "        self.tokenizer = REMI()\n",
    "        self.checkpoint_path = dict_pth\n",
    "        self.x_len = 512\n",
    "        self.dictionary_path = dict_pth\n",
    "        self.event2word, self.word2event = pickle.load(open(self.dictionary_path, 'rb'))\n",
    "        self.parser = self.prepare_data(self.midi_l)\n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.parser)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            return self.parser[index]\n",
    "        else:\n",
    "            return self.words\n",
    "            \n",
    "    def extract_events(self, input_path):\n",
    "        note_items, tempo_items = utils.read_items(input_path)\n",
    "        note_items = utils.quantize_items(note_items)\n",
    "        max_time = note_items[-1].end\n",
    "        if 'chord' in self.checkpoint_path:\n",
    "            chord_items = utils.extract_chords(note_items)\n",
    "            items = chord_items + tempo_items + note_items\n",
    "        else:\n",
    "            items = tempo_items + note_items\n",
    "        groups = utils.group_items(items, max_time)\n",
    "        events = utils.item2event(groups)\n",
    "        return events\n",
    "        \n",
    "    def prepare_data(self, midi_paths):\n",
    "        print(f'{len(midi_paths)} midis.')\n",
    "        # extract events\n",
    "        all_events = []\n",
    "        for path in midi_paths:\n",
    "            events = self.extract_events(path)\n",
    "            all_events.append(events)\n",
    "        print(f'{len(all_events)} events.')\n",
    "        # event to word\n",
    "        all_words = []\n",
    "        for events in all_events:\n",
    "            words = []\n",
    "            for event in events:\n",
    "                e = '{}_{}'.format(event.name, event.value)\n",
    "                if e in self.event2word:\n",
    "                    words.append(self.event2word[e])\n",
    "                else:\n",
    "                    # OOV\n",
    "                    if event.name == 'Note Velocity':\n",
    "                        words.append(self.event2word['Note Velocity_21'])\n",
    "                    else:\n",
    "                        print('something is wrong! {}'.format(e))\n",
    "            all_words.append(words)\n",
    "        # to training data\n",
    "        print(f'{len(all_words)} words.')\n",
    "        \n",
    "        segments = []\n",
    "        for words in all_words:\n",
    "            if len(words) < self.x_len + 1:\n",
    "                continue\n",
    "            step_size = self.x_len // 2\n",
    "            for i in range(0, len(words) - self.x_len, step_size):\n",
    "                if i + self.x_len + 1 <= len(words):\n",
    "                    x = words[i:i + self.x_len]\n",
    "                    y = words[i + 1:i + self.x_len + 1]\n",
    "                    segments.append([x, y])\n",
    "        segments = np.array(segments)\n",
    "        print(segments.shape)\n",
    "        return segments\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family:serif; font-size:20px;\"> 2.3.2 Model Architecture</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/imgs/vanilla.png\"/>\n",
    "\n",
    "*Figure 4: Illustration of the vanilla model with a segment length 4, from [Dai et.al, 2019](https://arxiv.org/pdf/1901.02860)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/imgs/transformer-xl.png\"/>\n",
    "\n",
    "*Figure 5: Illustration of the Transformer-XL model with a segment length 4, from [Dai et.al, 2019](https://arxiv.org/pdf/1901.02860)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model architecture, we utilize the huggingface implementation of [Transformer-XL](https://arxiv.org/pdf/1901.02860), which leverage a **segment-level recurrence mechanism** and a **relative positional encoding scheme** to enable the modeling of longer context.\n",
    "\n",
    "```python\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, checkpoint, is_training=False):\n",
    "        super(Model, self).__init__()\n",
    "        # load dictionary\n",
    "        self.dictionary_path = checkpoint\n",
    "        self.checkpoint_path = checkpoint\n",
    "        self.event2word, self.word2event = pickle.load(open(self.dictionary_path, 'rb'))\n",
    "        # model settings\n",
    "        self.x_len = 512\n",
    "        self.mem_len = 512\n",
    "        self.n_layer = 12\n",
    "        self.d_embed = 512\n",
    "        self.d_model = 512\n",
    "        self.dropout = 0.1\n",
    "        self.n_head = 8\n",
    "        self.d_head = self.d_model // self.n_head\n",
    "        self.d_ff = 2048\n",
    "        self.n_token = len(self.event2word)\n",
    "        self.learning_rate = 0.0002\n",
    "        # load model\n",
    "\n",
    "        self.configuration = TransfoXLConfig(\n",
    "            #...concrete config ommitted\n",
    "        )\n",
    "         \n",
    "        # Initializing a model (with random weights) from the configuration\n",
    "        self.xl = TransfoXLModel(self.configuration)\n",
    "        self.drop = nn.Dropout(p=self.dropout)\n",
    "        self.linear = nn.Linear(self.d_embed, self.n_token)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.xl(input_ids = x)\n",
    "        output = self.drop(outputs['last_hidden_state']) # dropout\n",
    "        output_logit = self.linear(output)\n",
    "        return output_logit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family:serif; font-size:20px;\"> 2.3.3 Training Loop</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each epoch, the model trains on batches, periodically validates on a held-out set, saves the best checkpoint, and stops early if validation does not improve for several epochs.\n",
    "\n",
    "```python\n",
    "for epoch in range(start_epoch, epochs+1):\n",
    "    # Training phase\n",
    "    single_epoch = []\n",
    "    for i in tqdm(train_dataloader, desc=f\"Epoch {epoch} Training\"):\n",
    "        x = i[:, 0, :].to(device).long()  \n",
    "        y = i[:, 1, :].to(device).long() \n",
    "            \n",
    "        output_logit = model(x)\n",
    "        # output_logit = (batch, 512, vocab_size)\n",
    "            \n",
    "        loss = nn.CrossEntropyLoss()(output_logit.permute(0,2,1), y)\n",
    "        loss.backward()\n",
    "        single_epoch.append(loss.to('cpu').mean().item())\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # Calculate training NLL for this epoch\n",
    "    train_epoch_nll = np.array(single_epoch).mean()\n",
    "    losses.append(train_epoch_nll)\n",
    "    train_nlls.append(train_epoch_nll)\n",
    "        \n",
    "    print(f'>>> Epoch: {epoch}, Train NLL: {train_epoch_nll:.5f}')\n",
    "        \n",
    "    # Validation phase\n",
    "    if epoch % opt.val_freq == 0 or epoch == epochs:\n",
    "        print(f\"Calculating validation NLL for epoch {epoch}...\")\n",
    "        val_nll, val_perplexity, val_tokens = calculate_nll(model, val_dataloader, device, max_batches=50)\n",
    "        val_nlls.append(val_nll)\n",
    "            \n",
    "        print(f'>>> Epoch: {epoch}, Validation NLL: {val_nll:.5f}, Perplexity: {val_perplexity:.2f}')\n",
    "        print(f'>>> Evaluated on {val_tokens} tokens')\n",
    "            \n",
    "        # Early stopping check\n",
    "        if val_nll < best_val_nll:\n",
    "            best_val_nll = val_nll\n",
    "            patience_counter = 0\n",
    "            print(f'>>> New best validation NLL: {best_val_nll:.5f}')\n",
    "            # Save best model\n",
    "            best_model_path = './checkpoints/best_model.pkl'\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict(),\n",
    "                    'train_nll': train_epoch_nll,\n",
    "                    'val_nll': val_nll,\n",
    "                    'best_val_nll': best_val_nll,\n",
    "                    'patience_counter': patience_counter,\n",
    "                    'train_nlls': train_nlls,\n",
    "                    'val_nlls': val_nlls,\n",
    "                }, best_model_path)\n",
    "            print(f'>>> Best model saved to {best_model_path}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'>>> No improvement. Patience: {patience_counter}/{opt.early_stop_patience}')\n",
    "                \n",
    "            if patience_counter >= opt.early_stop_patience:\n",
    "                print(f'>>> Early stopping triggered after {patience_counter} epochs without improvement')\n",
    "                break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family:serif; font-size:20px;\"> 2.3.4 Inference</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the inference stage, we autoregressively apply temperature sampling and concat the output token to input for next iteration.\n",
    "\n",
    "```python\n",
    "def temperature_sampling(logits, temperature, topk):\n",
    "        # probs = np.exp(logits / temperature) / np.sum(np.exp(logits / temperature))\n",
    "        logits = torch.Tensor(logits)\n",
    "        probs = nn.Softmax(dim=0)(logits / temperature)\n",
    "        probs = np.array(probs)\n",
    "        if topk == 1:\n",
    "            prediction = np.argmax(probs)\n",
    "        else:\n",
    "            sorted_index = np.argsort(probs)[::-1]\n",
    "            candi_index = sorted_index[:topk]\n",
    "            candi_probs = [probs[i] for i in candi_index]\n",
    "            # normalize probs\n",
    "            candi_probs /= sum(candi_probs)\n",
    "            # choose by predicted probs\n",
    "            prediction = np.random.choice(candi_index, size=1, p=candi_probs)[0]\n",
    "        return prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:serif; font-size:28px;\"> 3. Evaluation</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:24px;\"> 3.1 Context</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Metrics:**\n",
    "\n",
    "Following prior research on music generation ([Huang et al. 2018](https://arxiv.org/pdf/1809.04281); [Novack et al. 2024](https://arxiv.org/abs/2401.12179)), we adopt a range of objective metrics to evaluate our method, which fall into three main categories:\n",
    "\n",
    "- Learning-related metrics: **Negative log-likelihood (NLL)** on the validation set, which measures how well the model predicts musical events.\n",
    "\n",
    "- Model-based metrics: **[Fréchet Audio Distance (FAD)](https://arxiv.org/abs/1812.08466)**, which quantifies the similarity between the embeddings of generated and reference audio, providing a perceptual measure of generation quality.\n",
    "\n",
    "Due to resource constraints, we do not conduct subjective evaluation.\n",
    "\n",
    "**The properties of a “good” output:** A good output in unconditional music generation should exhibit musical coherence and structure, stylistic consistency with the training data, diversity, and creativity. The music should follow logical progressions in melody, harmony, and rhythm, and sound natural and engaging to human listeners, even though it is not conditioned on any specific prompt or input.\n",
    "\n",
    "**Relationship between the objective being optimized by your model and the evaluation metrics:** The model is optimized for sequence likelihood (e.g., negative log-likelihood), encouraging statistical similarity to the training data. However, low perplexity does not always ensure musical quality, so we also use perceptual measures like FAD to measure similarity to reference data. Ultimately, subjective evaluation remains necessary for a comprehensive assessment.\n",
    "\n",
    "\n",
    "\n",
    "<!-- What is the relationship between the objective being optimized by\n",
    "your model (e.g. perplexity) and musical properties (e.g. does it follow harmonic “rules”?) versus subjective properties? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:24px;\"> 3.2 Baseline Choices and Evaluation Protocol</span>\n",
    "\n",
    "**Baselines**\n",
    "\n",
    "We adopt two next-token prediction models, **Markov Chain and LSTM**, as baselines for comparison, as introduced in the previous section.\n",
    "\n",
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/imgs/LSTM_Cell.png\" width=\"500\"></img><img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/imgs/mc.png\" width=\"500\"></img>\n",
    "\n",
    "*Figure 6: illustration of LSTM (left) and Markov Chain (right)*\n",
    "\n",
    "**Evaluation Protocol**\n",
    "\n",
    "For each model, we select the checkpoint with the best validation negative log-likelihood and generate 500 samples. These samples are evaluated against the [POP909](https://arxiv.org/abs/2008.07142) dataset as the reference, using Fréchet Audio Distance (FAD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference Dataset Statistics**\n",
    "\n",
    "The POP909 dataset underwent extensive preprocessing to maximize its value for music research. Specifically, beat and chord annotations were extracted from MIDI and audio files using established music information retrieval algorithms from prior work. Key (tonality) annotations were also computed using existing MIR techniques, while tempo curves were carefully hand-labeled to ensure accuracy. By combining manual annotation with automated extraction methods, the dataset provides comprehensive and reliable musical features for each song.\n",
    "\n",
    "It is important to note that **POP909 was collected independently from POP1K7**, which serves as our training data. As a result, POP909 is well-suited for use as a reference dataset in our evaluation, given that it covers the same genre.\n",
    "\n",
    "The statistics is presented below:\n",
    "\n",
    "| Feature            | Mean     | Median   | Min     | Max     | Std      | N   |\n",
    "|--------------------|----------|----------|---------|---------|----------|-----|\n",
    "| pitch_density      | 0.0231   | 0.0221   | 0.0079  | 0.1429  | 0.0084   | 909 |\n",
    "| pitch_range        | 53.3465  | 53.0000  | 29.0000 | 80.0000 | 6.9464   | 909 |\n",
    "| avg_pitch_interval | 6.5809   | 6.4766   | 3.4998  | 11.0428 | 0.8275   | 909 |\n",
    "| note_density       | 6.9075   | 6.6670   | 3.4437  | 16.0498 | 1.6247   | 909 |\n",
    "| avg_ioi            | 0.1505   | 0.1489   | 0.0622  | 0.2885  | 0.0311   | 909 |\n",
    "\n",
    "\n",
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/pop909_analysis/avg_ioi.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/pop909_analysis/avg_pitch_interval.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/pop909_analysis/note_density.png\" width=\"350\"/>\n",
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/pop909_analysis/pitch_density.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/pop909_analysis/pitch_range.png\" width=\"350\"/> \n",
    "\n",
    "*Figure 7: Music Feature Distribution of  POP909.*\n",
    "\n",
    "Figure 8 shows the distributional differences between POP1K7 and POP909. While there is a notable difference in the average pitch interval, the distributions of most other features largely overlap. This suggests that these feature-based metrics are generally reasonable for evaluation. Nevertheless, perceptual metrics such as FAD remain important for a more thorough and comprehensive assessment.\n",
    "\n",
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/train_ref_diff/avg_ioi.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/train_ref_diff/avg_pitch_interval.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/train_ref_diff/note_density.png\" width=\"350\"/>\n",
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/train_ref_diff/pitch_density.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/train_ref_diff/pitch_range.png\" width=\"350\"/> \n",
    "\n",
    "*Figure 8: Music Feature Distributional Comparison between POP909 and POP1K7.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:24px;\"> 3.3 Codes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:22px;\"> 3.3.1 Implementation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative Log Likelihood**\n",
    "\n",
    "The average negative log-likelihood (NLL) per token is defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{NLL} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_\\theta(y_i \\mid x_i)\n",
    "$$\n",
    "\n",
    "where $N$ is the number of tokens, $y_i$ is the target token, and $p_\\theta(y_i \\mid x_i)$ is the predicted probability from the model.\n",
    "\n",
    "\n",
    "```python\n",
    "def calculate_nll(model, dataloader, device, max_batches=None):\n",
    "    \"\"\"Calculate negative log-likelihood on validation/test set\"\"\"\n",
    "    model.eval()\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Calculating NLL\")):\n",
    "            if max_batches and i >= max_batches:\n",
    "                break\n",
    "                \n",
    "            x = batch[:, 0, :].to(device).long() \n",
    "            y = batch[:, 1, :].to(device).long()\n",
    "            \n",
    "            output_logit = model(x)\n",
    "            \n",
    "            # Calculate cross-entropy loss (which is negative log-likelihood)\n",
    "            loss = nn.CrossEntropyLoss(reduction='sum')(output_logit.permute(0,2,1), y)\n",
    "            \n",
    "            total_nll += loss.item()\n",
    "            total_tokens += y.numel()  # Total number of tokens\n",
    "            batch_count += 1\n",
    "    \n",
    "    avg_nll = total_nll / total_tokens if total_tokens > 0 else float('inf')\n",
    "    perplexity = torch.exp(torch.tensor(avg_nll)).item()\n",
    "    \n",
    "    model.train()  # Set back to training mode\n",
    "    return avg_nll, perplexity, total_tokens\n",
    "```\n",
    "\n",
    "**Fréchet Audio Distance**\n",
    "\n",
    "Given two sets of embeddings with means $\\mu_1, \\mu_2$ and covariances $\\Sigma_1, \\Sigma_2$, the Fréchet distance is:\n",
    "\n",
    "$$\n",
    "\\mathrm{FAD} = \\|\\mu_1 - \\mu_2\\|^2 + \\mathrm{Tr}\\left(\\Sigma_1 + \\Sigma_2 - 2(\\Sigma_1 \\Sigma_2)^{1/2}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "```python\n",
    "def extract_clap_embeddings(audio_paths, processor, model, device, bs=8):\n",
    "    \"\"\"Return np.array shape (N, embed_dim)\"\"\"\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(audio_paths), bs), desc=\"Embedding\"):\n",
    "        batch_files = audio_paths[i:i+bs]\n",
    "        batch_waveforms = [load_audio(path) for path in batch_files]\n",
    "        inputs = processor(audios=batch_waveforms, return_tensors=\"pt\", sampling_rate=48000, padding=True)\n",
    "        with torch.no_grad():\n",
    "            input_tensor = inputs[\"input_features\"].to(device)\n",
    "            output = model.get_audio_features(input_tensor)\n",
    "            emb = output.cpu().numpy()\n",
    "        embeddings.append(emb)\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    return embeddings\n",
    "\n",
    "def compute_fad(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Fréchet distance for two Gaussians.\"\"\"\n",
    "    covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    # numerical stability\n",
    "    if not np.isfinite(covmean).all():\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    diff = mu1 - mu2\n",
    "    return diff.dot(diff) + np.trace(sigma1 + sigma2 - 2*covmean)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:22px;\"> 3.3.2 Evaluation Result</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "| Model        | Val NLL  | FAD      |\n",
    "|--------------|:--------:|:--------:|\n",
    "| Markov Chain | 2.9434     | 0.1398   |\n",
    "| LSTM         | 1.2968    |  0.1145  |\n",
    "| Pop Music Transformer | **1.2154** |  **0.0822**   |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of Generated Samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot the distributions of musical features for the generated MIDI files produced by each method. The differences between these distributions and those of the reference set are quantified using KL divergence and Wasserstein distance.\n",
    "\n",
    "The KL divergence between two discrete distributions $P$ and $Q$ is:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\,\\|\\, Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "For 1-dimensional distributions $P$ and $Q$, the Wasserstein (first-order) distance is:\n",
    "\n",
    "$$\n",
    "W(P, Q) = \\int_{-\\infty}^{\\infty} | F_P(x) - F_Q(x) |\\, dx\n",
    "$$\n",
    "\n",
    "where $F_P(x)$ and $F_Q(x)$ are the cumulative distribution functions of $P$ and $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov Chain**\n",
    "\n",
    "\n",
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/mc_eval/avg_ioi.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/mc_eval/avg_pitch_interval.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/mc_eval/note_density.png\" width=\"350\"/>\n",
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/mc_eval/pitch_density.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/mc_eval/pitch_range.png\" width=\"350\"/> \n",
    "\n",
    "*Figure 9: Music Feature Distributional Comparison between MC-generated samples and POP909.*\n",
    "\n",
    "**LSTM**\n",
    "\n",
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/lstm_eval/avg_ioi.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/lstm_eval/avg_pitch_interval.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/lstm_eval/note_density.png\" width=\"350\"/>\n",
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/lstm_eval/pitch_density.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/lstm_eval/pitch_range.png\" width=\"350\"/> \n",
    "\n",
    "*Figure 10: Music Feature Distributional Comparison between LSTM-generated samples and POP909.*\n",
    "\n",
    "**Pop Music Transformer**\n",
    "\n",
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/popmt_eval/avg_ioi.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/popmt_eval/avg_pitch_interval.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/popmt_eval/note_density.png\" width=\"350\"/>\n",
    "<img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/popmt_eval/pitch_density.png\" width=\"350\"/> <img src=\"https://anonymous.4open.science/r/remi-pytorch-6C7D/popmt_eval/pitch_range.png\" width=\"350\"/> \n",
    "\n",
    "*Figure 11: Music Feature Distributional Comparison between Transformer-generated samples and POP909.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:serif; font-size:28px;\"> 4. Related works</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:24px;\"> 4.1 Previous Dataset Usage</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets we use, POP1K7 and POP909, have been widely adopted in previous research.\n",
    "\n",
    "For example, POP909 has been used in studies on symbolic music classification ([Chou and Chen, 2024](https://arxiv.org/pdf/2107.05223v2)), symbolic music generation ([Choi and Lee, 2023](https://arxiv.org/pdf/2211.00895v2), [Shin et.al, 2022](https://arxiv.org/pdf/2111.04093v2)), and music representation/tokenization ([Fradet et al., 2023](https://arxiv.org/pdf/2301.11975v3)).  POP1K7 is primarily employed for tasks related to symbolic music generation([Zhang et al., 2022](https://arxiv.org/pdf/2211.00222)) and understanding ([Liang et al., 2024](https://arxiv.org/abs/2407.03361)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:24px;\"> 4.2 Other Work on Related Tasks</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the previous section, piano composition can be formulated as a sequential modeling problem. Accordingly, sequential models have been predominantly adopted for this task.\n",
    "\n",
    "[Oore and Simon (a), 2017](https://magenta.tensorflow.org/performance-rnn) and [Oore and Simon (b), 2020](https://arxiv.org/abs/1808.03715) employed recurrent neural networks (RNNs) to compose polyphonic music with expressive timing and dynamics.\n",
    "\n",
    "[Huang et al., 2018](https://arxiv.org/pdf/1809.04281) introduced a Transformer-based approach with enhanced sequential modeling capabilities for music generation. In a later work, [Huang et al., 2020](https://dl.acm.org/doi/pdf/10.1145/3394171.3413671) proposed the REMI tokenization scheme, further improving upon vanilla Transformer models for symbolic music tasks.\n",
    "\n",
    "[Muhamed et al., 2021](https://cdn.aaai.org/ojs/16117/16117-13-19611-1-2-20210518.pdf) incorporated adversarial loss from Generative Adversarial Networks (GANs) to improve the ability of Transformer models to generate long-term consistent music sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:serif; font-size:24px;\"> 4.3 Results Comparison to Related Work</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, our results are consistent with findings reported in previous research ([Huang et al., 2018](https://arxiv.org/pdf/1809.04281)), which demonstrate that transformer-based methods offer significant improvements over traditional baselines such as Markov Chain models and RNN-based models in both validation NLL and FAD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
